[connectivity]
# 3-stage streaming pipeline: load → minirocket_inference → store
# Number of compute units for each kernel
nk=load_kernel:1
nk=minirocket_inference:1
nk=store_kernel:1

# Stream connections between kernels
# load_kernel.output → minirocket_inference.input_timeseries
sc=load_kernel_1.output:minirocket_inference_1.input_timeseries
# minirocket_inference.output_predictions → store_kernel.input
sc=minirocket_inference_1.output_predictions:store_kernel_1.input

# HBM port assignments for load_kernel
sp=load_kernel_1.time_series_input:HBM[0]

# HBM port assignments for minirocket_inference (model parameters)
sp=minirocket_inference_1.coefficients:HBM[2]
sp=minirocket_inference_1.intercept:HBM[3]
sp=minirocket_inference_1.scaler_mean:HBM[4]
sp=minirocket_inference_1.scaler_scale:HBM[5]
sp=minirocket_inference_1.dilations:HBM[6]
sp=minirocket_inference_1.num_features_per_dilation:HBM[7]
sp=minirocket_inference_1.biases:HBM[8]

# HBM port assignments for store_kernel
sp=store_kernel_1.predictions_output:HBM[1]